{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpowAC5jvFpdMctzVIfCBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzlilLV97/Deep-Learning-Final-Project/blob/main/V1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Project: \n",
        "\n",
        "Deep learning Course\n",
        "\n",
        "Tzlil Lev or 318646510\n",
        "\n",
        "Alon Feldman - too old to have id number\n",
        "\n",
        "Project Name : Predict League of Legends victory \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pouqnjo5IbYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykm8lpMmIV3J",
        "outputId": "402c8fd7-826b-4036-ace6-86d1bfa789a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Path to save the dataset.\n",
        "PATH_TO_DATA = '/content/gdrive/MyDrive/Colab Notebooks/data/games.csv' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preperation \n"
      ],
      "metadata": {
        "id": "HhYEIg_19EG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def data_preparation(df_first):\n",
        "    df = df_first.iloc[:, [2, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 23, 26, 27, 28, 29, 30, 36, 39, 42, 45, 48,  51, 52, 53, 54, 55]]\n",
        "    ## add 2, 5\n",
        "    ## NORMALIZE THE GAME DURATION\n",
        "    df['gameDuration'] = df['gameDuration'].apply(lambda x: np.log(x))\n",
        "    # print(df['gameDuration'])\n",
        "    labels = df_first.iloc[:, 4].to_numpy() - 1\n",
        "    df = df.to_numpy()\n",
        "    return df, labels\n",
        "\n",
        "\n",
        "def get_batch(data,labels, start, end):\n",
        "    x = data[start:end]\n",
        "    s = labels[start:end]\n",
        "    return x, s\n",
        "\n",
        "df = pd.read_csv(PATH_TO_DATA)\n",
        "data, labels = data_preparation(df)\n",
        "n = len(data)\n",
        "len_train = int(0.8 * n)\n",
        "len_test = (n-int(len_train))//2\n",
        "len_val = len_test\n",
        "train, valid, test = (data[:len_train],labels[:len_train]), (data[len_train:len_train+len_test],labels[len_train:len_train+len_test]), (data[len_train+len_test:],labels[len_train+len_test:])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9pWq3gTr9GZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646a446a-99ae-40fa-a2f1-0a49567559c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-47b5147fee2e>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['gameDuration'] = df['gameDuration'].apply(lambda x: np.log(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BENCHMARK**\n",
        "\n",
        "In order to evalute our model, we compared it to different models, to \n",
        "\n",
        "observe the results and compare them to our network. \n",
        "\n",
        "We choose the following models:\n",
        "\n",
        "\n",
        "* XGBoost\n",
        "\n",
        "* Random Forest\n",
        "\n",
        "* KNN\n",
        "\n",
        "* SVM\n",
        "\n",
        "* Random Forest\n",
        "\n",
        "The Following Code is the implementation of each of them\n"
      ],
      "metadata": {
        "id": "qoiADTCnI1gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SVM \n",
        "\n",
        "def estimate_accuracy_svm(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        # forward pass prediction\n",
        "        y = model.predict(xt)\n",
        "        pred = np.squeeze(y)\n",
        "        correct += np.sum(pred==st)\n",
        "        N += len(st)\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "\n",
        "\n",
        "def svmModel(train, valid, test):\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    svm_model = svm.SVC()\n",
        "    svm_model.fit(x_train, y_train)\n",
        "    y_pred = svm_model.predict(x_test)\n",
        "    print(\"SVM Accuracy on validation set: {:.2f}%\".format(estimate_accuracy_svm(svm_model, x_valid, y_valid) * 100))\n",
        "    print(\"SVM Accuracy on test set: {:.2f}%\".format(estimate_accuracy_svm(svm_model, x_test, y_test) * 100))\n",
        "\n",
        "\n",
        "svmModel(train, valid, test)\n",
        "         \n",
        "\n"
      ],
      "metadata": {
        "id": "gtV8jmQ7JFAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342a7e13-a6a0-41ae-cbbe-af9d69e47b03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy on validation set: 94.08%\n",
            "SVM Accuracy on test set: 93.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### KNN\n",
        "def estimate_accuracy_KNN(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        # forward pass prediction\n",
        "        y = model.predict(xt)\n",
        "        pred = np.squeeze(y)\n",
        "        correct += np.sum(pred==st)\n",
        "        N += len(st)\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def KNN(train,valid,test):\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    model = KNeighborsClassifier(n_neighbors=5)\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    print(\"KNN Accuracy on validation set: {:.2f}%\".format(estimate_accuracy_KNN(model, x_valid, y_valid) * 100))\n",
        "    print(\"KNN Accuracy on test set: {:.2f}%\".format(estimate_accuracy_KNN(model, x_test, y_test) * 100))\n",
        "\n",
        "KNN(train, valid, test)"
      ],
      "metadata": {
        "id": "ejVkFTy1Jcid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78e6f01-8689-4526-86dc-2d73926d26ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy on validation set: 50.42%\n",
            "KNN Accuracy on test set: 52.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### XGBoost\n",
        "\n",
        "def compare_xgb_predictions(xgb_model, X, y):\n",
        "    # Make predictions using the trained model\n",
        "    global n, y_test,x_test,x_valid,y_valid\n",
        "    xgb_predictions = xgb_model.predict(X)\n",
        "\n",
        "    # Round the predictions to 0 or 1\n",
        "    xgb_predictions = [round(pred) for pred in xgb_predictions]\n",
        "\n",
        "    # Compare the predictions with the real labels\n",
        "    correct = 0\n",
        "    total = len(X)\n",
        "    for pred, label in zip(xgb_predictions, y):\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    # Calculate the accuracy of the predictions\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def xgb_classifier(train,valid,test):\n",
        "    global n, y_test,x_test,x_valid,y_valid\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    #  Write your code here\n",
        "    model = xgb.XGBClassifier()\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "  #  print(classification_report(y_test, y_pred))\n",
        "    print(\"XGB Accuracy on validation set: {:.2f}%\".format(compare_xgb_predictions(model, x_valid, y_valid) * 100))\n",
        "    print(\"XGB Accuracy on test set: {:.2f}%\".format(compare_xgb_predictions(model, x_test, y_test) * 100))\n",
        "          #\"compare_xgb_predictions(model, x_test, y_test))\n",
        "\n",
        "xgb_classifier(train, valid, test)"
      ],
      "metadata": {
        "id": "Ir-gjlfpJiaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5867c32d-b90c-4c34-846e-f7dfbd2a3c30"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB Accuracy on validation set: 97.28%\n",
            "XGB Accuracy on test set: 97.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "t1xX0b7Z-FK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest(train_data, validation_data, test_data):\n",
        "    # Create a random forest classifier\n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(train_data[0], train_data[1])\n",
        "\n",
        "    # Predict the labels of the test set\n",
        "    preds = clf.predict(validation_data[0])\n",
        "\n",
        "    # Compute the accuracy: accuracy\n",
        "    accuracy = accuracy_score(validation_data[1], preds)\n",
        "    print(\"Random Forest Accuracy on Validation Set: %.2f%%\" % (accuracy * 100))\n",
        "    print(\"Random Forest Accuracy on Training Set: %.2f%%\" % (accuracy_score(train_data[1], clf.predict(train_data[0])) * 100))\n",
        "    print(\"Random Forest Accuracy on Test Set: %.2f%%\" % (accuracy_score(test_data[1], clf.predict(test_data[0])) * 100))\n",
        "\n",
        "random_forest(train, valid, test)"
      ],
      "metadata": {
        "id": "cK_qpBBk-GQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy estimation "
      ],
      "metadata": {
        "id": "5jEgvJKGKeeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def estimate_accuracy_torch(model, data, labels, batch_size=5000, max_N=100000):\n",
        "  \"\"\"\n",
        "  Estimate the accuracy of the model on the data. To reduce\n",
        "  computation time, use at most `max_N` elements of `data` to\n",
        "  produce the estimate.\n",
        "  \"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  N = 0\n",
        "  for i in range(0, len(data), batch_size):\n",
        "      # get a batch of data\n",
        "      xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "      # forward pass prediction\n",
        "      y = model(torch.Tensor(xt))\n",
        "      y = y.detach().numpy()  # convert the PyTorch tensor => numpy array\n",
        "      y = np.where(y > 0.5, 1, 0)\n",
        "      pred = np.squeeze(y)\n",
        "      correct += np.sum(pred==st)\n",
        "      N += len(st)\n",
        "\n",
        "      if N > max_N:\n",
        "          break\n",
        "  return correct / N\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "i1qED1TEKfuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "maXUOsQY-x7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    def _init_(self, num_hidden=5):\n",
        "        super(PyTorchMLP, self)._init_()\n",
        "        self.input_size = 27  # input_size\n",
        "        self.hidden_size = 50 # hidden_size was 50\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc5 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc6 = torch.nn.Linear(self.hidden_size, 1)\n",
        "       # self.dropout = torch.nn.Dropout(0.1)\n",
        "       # self.optimizer2 = torch.optim.RMSprop(self.parameters())\n",
        "       #  self.sigmoid = F.sigmoid()\n",
        "       #  self.tanh = F.tanh()\n",
        "        self.normalizer = torch.nn.GroupNorm(1, self.hidden_size)\n",
        "    def forward(self, inp):\n",
        "        hidden1 = self.fc1(inp)\n",
        "        relu1 = F.relu(hidden1)\n",
        "  #      bypass = self.fc6(relu1)\n",
        "        hidden2 = self.fc2(relu1)\n",
        "        # Add bypass branch\n",
        "        bypass = self.fc6(hidden1)\n",
        "        relu3 = F.sigmoid(hidden2)\n",
        "        output = self.fc6(relu3)\n",
        "        # Combine bypass and regular outputs\n",
        "        #output = F.sigmoid(bypass+ output)\n",
        "        output = F.sigmoid(output)\n",
        "        return output\n",
        "    # def forward(self, inp):\n",
        "    #     hidden1 = self.fc1(inp)\n",
        "    #     relu1 = self.relu(hidden1)\n",
        "    #     ##ADD BYPASS BRANCH\n",
        "    #     bypass = self.fc6(relu1)\n",
        "    #     hidden2 = self.fc2(relu1)\n",
        "    #     relu2 = self.relu(hidden2)\n",
        "    #     hidden3 = self.fc3(relu2)\n",
        "    #     relu3 = self.relu(hidden3)\n",
        "    #     output = self.fc6(relu3)\n",
        "    #     ##COMBINE BYPASS AND REGULAR OUTPUTS\n",
        "    #     return self.sigmoid(output+bypass)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "-X6zH0SJ-h1r",
        "outputId": "99ebe3fc-d285-41d6-949d-af7f7bcbb20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cd4950250869>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPyTorchMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyTorchMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m27\u001b[0m  \u001b[0;31m# input_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;31m# hidden_size was 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_accuracy_torch(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        # forward pass prediction\n",
        "        y = model(torch.Tensor(xt))\n",
        "        y = y.detach().numpy()  # convert the PyTorch tensor => numpy array\n",
        "        y = np.where(y > 0.5, 1, 0)\n",
        "        pred = np.squeeze👍\n",
        "        correct += np.sum(pred==st)\n",
        "        N += len(st)\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "  \"\"\"\n",
        "  Plot the learning curve.\n",
        "  \"\"\"\n",
        "  plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "  plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "  plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "def make_prediction_torch(model, game_data):\n",
        "\n",
        "  b = game_data\n",
        "  toBePredicted = torch.Tensor(b)\n",
        "  print(toBePredicted)\n",
        "  output = model(toBePredicted)\n",
        "  print(output)\n",
        "  #  Write your code here\n",
        "\n",
        "def pytorch_gradient_descent(model, train_data,\n",
        "                                validation_data,test_data,\n",
        "                                batch_size=100,\n",
        "                                learning_rate=0.001,\n",
        "                                weight_decay=0,\n",
        "                                epochs=1000):\n",
        "  ##COSTS\n",
        "  #criterion =nn.BCELoss()# nn.CrossEntropyLoss()\n",
        "  #criterion = nn.NLLLoss()\n",
        "  criterion = nn.MSELoss()\n",
        "  #criterion = nn.CrossEntropyLoss()\n",
        "  ##EXTRA FEATURES\n",
        "  # droper = nn.Dropout(p=0.1)\n",
        "  droper = 0\n",
        "\n",
        "  ## OPTIMIZER\n",
        "  #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  iters, losses = [], []\n",
        "  iters_sub, train_accs, val_accs = [], [], []\n",
        "\n",
        "  n = 0  # the number of iterations\n",
        "  while True:\n",
        "      for i in range(0, train_data[0].shape[0], batch_size):\n",
        "          if (i + batch_size) > train_data[0].shape[0]:\n",
        "              break\n",
        "          # get the input and targets of a minibatch\n",
        "          xt, st = get_batch(train_data[0],train_data[1], i, i + batch_size)\n",
        "          ## Create normalized data\n",
        "          # xt,mio,sigma = probabilistic_normalization(xt)\n",
        "          # convert from numpy arrays to PyTorch tensors\n",
        "          xt = torch.Tensor(xt)\n",
        "          st = st\n",
        "          st = torch.Tensor(st).float().unsqueeze(1)\n",
        "          if droper:\n",
        "              zs = model(droper(xt)) # compute prediction logit, use dropout if wanted\n",
        "          else:\n",
        "              zs = model(xt)\n",
        "          loss = criterion(st, zs)  # compute the total loss\n",
        "          loss.backward()  # compute updates for each parameter\n",
        "          optimizer.step()  # make the updates for each parameter\n",
        "          optimizer.zero_grad()  # a clean up step for PyTorch\n",
        "          # save the current training information\n",
        "          iters.append👎\n",
        "          losses.append(float(loss) / batch_size)  # compute average loss\n",
        "\n",
        "          if n % 10 == 0:\n",
        "          #    print(classification_report(zs.detach().numpy(),st.detach().numpy()))\n",
        "          #    continue\n",
        "              iters_sub.append👎\n",
        "              train_cost = float(loss.detach().numpy())\n",
        "              train_acc = estimate_accuracy_torch(model, train_data[0],train_data[1])\n",
        "              train_accs.append(train_acc)\n",
        "              val_acc = estimate_accuracy_torch(model, validation_data[0], validation_data[1])\n",
        "              val_accs.append(val_acc)\n",
        "          #    print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "          #        n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "          # increment the iteration number\n",
        "      n += 1\n",
        "\n",
        "      if n > epochs:\n",
        "          train_cost = float(loss.detach().numpy())\n",
        "          train_acc = estimate_accuracy_torch(model, train_data[0], train_data[1])\n",
        "          train_accs.append(train_acc)\n",
        "          val_acc = estimate_accuracy_torch(model,validation_data[0], validation_data[1])\n",
        "          val_accs.append(val_acc)\n",
        "          # print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "          #     n, val_acc * 100, train_acc * 100, train_cost))\n",
        "          print(\"NN Accuracy on Validation Set: %.2f%%\" % (val_acc * 100))\n",
        "          print(\"NN Accuracy on Training Set: %.2f%%\" % (train_acc * 100))\n",
        "          print(\"NN Accuracy on Test Set: %.2f%%\" % (estimate_accuracy_torch(model, test_data[0], test_data[1]) * 100))\n",
        "          return iters, losses, iters_sub, train_accs, val_accs\n"
      ],
      "metadata": {
        "id": "15pBOmcH-4Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5AsXT23Z-9Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_mlp = PyTorchMLP()\n",
        "learning_curve_info = pytorch_gradient_descent(pytorch_mlp, train,valid ,test,batch_size=5000,\n",
        "                            learning_rate=0.00013,\n",
        "                            weight_decay=0.0000000,\n",
        "                            epochs=400)"
      ],
      "metadata": {
        "id": "fkA8Z3bb-9zp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}