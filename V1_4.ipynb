{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7pnUphw3JwjNtR6putK2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tzlilLV97/Deep-Learning-Final-Project/blob/main/V1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Project: \n",
        "\n",
        "Deep learning Course\n",
        "\n",
        "Tzlil Lev or 318646510\n",
        "\n",
        "Alon Feldman - too old to have id number\n",
        "\n",
        "Project Name : Predict League of Legends victory \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pouqnjo5IbYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykm8lpMmIV3J",
        "outputId": "d1fdb948-5388-4a8f-d82e-92724119e82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Path to save the dataset.\n",
        "PATH_TO_DATA = '/content/gdrive/MyDrive/Colab Notebooks/data/games.csv' "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preperation \n"
      ],
      "metadata": {
        "id": "HhYEIg_19EG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preparation(df_first):\n",
        "    df = df_first.iloc[:, [2, 5, 6, 7, 8, 9, 10, 11, 14, 17, 20, 23, 26, 27, 28, 29, 30, 36, 39, 42, 45, 48,  51, 52, 53, 54, 55]]\n",
        "    ## add 2, 5\n",
        "    ## NORMALIZE THE GAME DURATION\n",
        "    #df['gameDuration'] = df['gameDuration'].apply(lambda x: np.log(x))\n",
        "    labels = df_first.iloc[:, 4].to_numpy() - 1\n",
        "    #Make it one hot\n",
        "    labels = [np.eye(2)[label] for label in labels]\n",
        "    df = df.to_numpy()\n",
        "    indices = np.arange(len(df))\n",
        "    np.random.shuffle(indices)\n",
        "    df = df[indices]\n",
        "    labels = np.array(labels)[indices]\n",
        "    return df, labels\n",
        "\n",
        "\n",
        "def get_batch(data,labels, start, end):\n",
        "    try:\n",
        "      x = data[start:end]\n",
        "      s = labels[start:end]\n",
        "      return x, s\n",
        "    except:\n",
        "      x = data[start:]\n",
        "      s = labels[start:]\n",
        "      return x,s\n",
        "\n",
        "\n",
        "df = pd.read_csv(PATH_TO_DATA)\n",
        "data, labels = data_preparation(df)\n",
        "n = len(data)\n",
        "len_train = int(0.8 * n)\n",
        "len_test = (n-int(len_train))//2\n",
        "len_val = len_test\n",
        "train, valid, test = (data[:len_train],labels[:len_train]), (data[len_train:len_train+len_test],labels[len_train:len_train+len_test]), (data[len_train+len_test:],labels[len_train+len_test:])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9pWq3gTr9GZT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BENCHMARK**\n",
        "\n",
        "In order to evalute our model, we compared it to different models, to \n",
        "\n",
        "observe the results and compare them to our network. \n",
        "\n",
        "We choose the following models:\n",
        "\n",
        "\n",
        "* XGBoost\n",
        "\n",
        "* Random Forest\n",
        "\n",
        "* KNN\n",
        "\n",
        "* SVM\n",
        "\n",
        "* Random Forest\n",
        "\n",
        "The Following Code is the implementation of each of them\n"
      ],
      "metadata": {
        "id": "qoiADTCnI1gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SVM \n",
        "\n",
        "def estimate_accuracy_svm(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        # forward pass prediction\n",
        "        y = model.predict(xt)\n",
        "        pred = np.squeeze(y)\n",
        "        correct += np.sum(pred==st)\n",
        "        N += len(st)\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "\n",
        "\n",
        "def svmModel(train, valid, test):\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    y_train = np.argmax(y_train, axis=1)\n",
        "    y_valid = np.argmax(y_valid, axis=1)\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "    svm_model = svm.SVC()\n",
        "    svm_model.fit(x_train, y_train)\n",
        "    y_pred = svm_model.predict(x_test)\n",
        "    print(\"SVM Accuracy on validation set: {:.2f}%\".format(estimate_accuracy_svm(svm_model, x_valid, y_valid) * 100))\n",
        "    print(\"SVM Accuracy on test set: {:.2f}%\".format(estimate_accuracy_svm(svm_model, x_test, y_test) * 100))\n",
        "\n",
        "\n",
        "svmModel(train, valid, test)\n",
        "         \n",
        "\n"
      ],
      "metadata": {
        "id": "gtV8jmQ7JFAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9245cc24-3bb4-4473-83d0-cdd46f492a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy on validation set: 84.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### KNN\n",
        "def estimate_accuracy_KNN(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        # forward pass prediction\n",
        "        y = model.predict(xt)\n",
        "        pred = np.squeeze(y)\n",
        "        correct += np.sum(pred==st)\n",
        "        N += len(st)\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def KNN(train,valid,test):\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    model = KNeighborsClassifier(n_neighbors=5)\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    print(\"KNN Accuracy on validation set: {:.2f}%\".format(estimate_accuracy_KNN(model, x_valid, y_valid) * 100))\n",
        "    print(\"KNN Accuracy on test set: {:.2f}%\".format(estimate_accuracy_KNN(model, x_test, y_test) * 100))\n",
        "\n",
        "KNN(train, valid, test)"
      ],
      "metadata": {
        "id": "ejVkFTy1Jcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### XGBoost\n",
        "\n",
        "def compare_xgb_predictions(xgb_model, X, y):\n",
        "    # Make predictions using the trained model\n",
        "    global n, y_test,x_test,x_valid,y_valid\n",
        "    xgb_predictions = xgb_model.predict(X)\n",
        "\n",
        "    # Round the predictions to 0 or 1\n",
        "    xgb_predictions = [round(pred) for pred in xgb_predictions]\n",
        "\n",
        "    # Compare the predictions with the real labels\n",
        "    correct = 0\n",
        "    total = len(X)\n",
        "    for pred, label in zip(xgb_predictions, y):\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "    # Calculate the accuracy of the predictions\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def xgb_classifier(train,valid,test):\n",
        "    global n, y_test,x_test,x_valid,y_valid\n",
        "    x_train, y_train = train\n",
        "    x_valid, y_valid = valid\n",
        "    x_test, y_test = test\n",
        "    #  Write your code here\n",
        "    model = xgb.XGBClassifier()\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "  #  print(classification_report(y_test, y_pred))\n",
        "    print(\"XGB Accuracy on validation set: {:.2f}%\".format(compare_xgb_predictions(model, x_valid, y_valid) * 100))\n",
        "    print(\"XGB Accuracy on test set: {:.2f}%\".format(compare_xgb_predictions(model, x_test, y_test) * 100))\n",
        "          #\"compare_xgb_predictions(model, x_test, y_test))\n",
        "\n",
        "xgb_classifier(train, valid, test)"
      ],
      "metadata": {
        "id": "Ir-gjlfpJiaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "t1xX0b7Z-FK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest(train_data, validation_data, test_data):\n",
        "    # Create a random forest classifier\n",
        "    clf = RandomForestClassifier(n_estimators=200, max_depth=12, random_state=0)\n",
        "\n",
        "    # Train the classifier\n",
        "    clf.fit(train_data[0], train_data[1])\n",
        "\n",
        "    # Predict the labels of the test set\n",
        "    preds = clf.predict(validation_data[0])\n",
        "\n",
        "    # Compute the accuracy: accuracy\n",
        "    accuracy = accuracy_score(validation_data[1], preds)\n",
        "    print(\"Random Forest Accuracy on Validation Set: %.2f%%\" % (accuracy * 100))\n",
        "    print(\"Random Forest Accuracy on Training Set: %.2f%%\" % (accuracy_score(train_data[1], clf.predict(train_data[0])) * 100))\n",
        "    print(\"Random Forest Accuracy on Test Set: %.2f%%\" % (accuracy_score(test_data[1], clf.predict(test_data[0])) * 100))\n",
        "\n",
        "def accuracy_score(y_true, y_pred):\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "random_forest(train, valid, test)"
      ],
      "metadata": {
        "id": "cK_qpBBk-GQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy estimation "
      ],
      "metadata": {
        "id": "5jEgvJKGKeeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_accuracy_torch(model, data, labels, batch_size=5000, max_N=100000):\n",
        "  \"\"\"\n",
        "  Estimate the accuracy of the model on the data. To reduce\n",
        "  computation time, use at most `max_N` elements of `data` to\n",
        "  produce the estimate.\n",
        "  \"\"\"\n",
        "\n",
        "  correct = 0\n",
        "  N = 0\n",
        "  for i in range(0, len(data), batch_size):\n",
        "      # get a batch of data\n",
        "      xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "      # forward pass prediction\n",
        "      y = model(torch.Tensor(xt))\n",
        "      y = y.detach().numpy()  # convert the PyTorch tensor => numpy array\n",
        "      y = np.where(y > 0.5, 1, 0)\n",
        "      pred = np.squeeze(y)\n",
        "      correct += np.sum(pred==st)\n",
        "      N += len(st)\n",
        "\n",
        "      if N > max_N:\n",
        "          break\n",
        "  return correct / N\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "i1qED1TEKfuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "maXUOsQY-x7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PyTorchMLP2(nn.Module):\n",
        "    def _init_(self, num_hidden=5):\n",
        "        super(PyTorchMLP, self)._init_()\n",
        "        self.input_size = 27  # input_size\n",
        "        self.hidden_size = 50 # hidden_size was 50\n",
        "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc4 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc5 = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.fc6 = torch.nn.Linear(self.hidden_size, 1)\n",
        "       # self.dropout = torch.nn.Dropout(0.1)\n",
        "       # self.optimizer2 = torch.optim.RMSprop(self.parameters())\n",
        "       #  self.sigmoid = F.sigmoid()\n",
        "       #  self.tanh = F.tanh()\n",
        "        self.normalizer = torch.nn.GroupNorm(1, self.hidden_size)\n",
        "    def forward(self, inp):\n",
        "        hidden1 = self.fc1(inp)\n",
        "        relu1 = F.relu(hidden1)\n",
        "  #      bypass = self.fc6(relu1)\n",
        "        hidden2 = self.fc2(relu1)\n",
        "        # Add bypass branch\n",
        "        bypass = self.fc6(hidden1)\n",
        "        relu3 = F.sigmoid(hidden2)\n",
        "        output = self.fc6(relu3)\n",
        "        output = F.sigmoid(output)\n",
        "        return output\n",
        "    # def forward(self, inp):\n",
        "    #     hidden1 = self.fc1(inp)\n",
        "    #     relu1 = self.relu(hidden1)\n",
        "    #     ##ADD BYPASS BRANCH\n",
        "    #     bypass = self.fc6(relu1)\n",
        "    #     hidden2 = self.fc2(relu1)\n",
        "    #     relu2 = self.relu(hidden2)\n",
        "    #     hidden3 = self.fc3(relu2)\n",
        "    #     relu3 = self.relu(hidden3)\n",
        "    #     output = self.fc6(relu3)\n",
        "    #     ##COMBINE BYPASS AND REGULAR OUTPUTS\n",
        "    #     return self.sigmoid(output+bypass)\n"
      ],
      "metadata": {
        "id": "-X6zH0SJ-h1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting Function"
      ],
      "metadata": {
        "id": "e0gLJ5H2GzQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "def plot_learning_curve(model_data,cost_label=False):\n",
        "  \"\"\"\n",
        "  Plot the learning curve.\n",
        "  \"\"\"\n",
        "  iters, losses, iters_sub, train_accs, val_accs = model_data\n",
        "  if cost_label:\n",
        "    plt.title(\"Learning Curve for Cost Function: \"+cost_label)\n",
        "  else:\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "  plt.plot(iters, losses, label=\"Train\")\n",
        "  plt.xlabel(\"Iterations\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "  plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "  plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "n-YdRsIAG0e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_accuracy_torch(model, data, labels, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        # get a batch of data\n",
        "        xt, st = get_batch(data, labels, i, i + batch_size)\n",
        "        xt = torch.Tensor(xt)\n",
        "        st = torch.Tensor(st)\n",
        "        y = model(xt)\n",
        "        _, pred = torch.max(y, 1)\n",
        "        _, true = torch.max(st, 1)\n",
        "        correct += (pred == true).sum().item()\n",
        "        N += len(st)\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def make_prediction_torch(model, game_data):\n",
        "\n",
        "  b = game_data\n",
        "  toBePredicted = torch.Tensor(b)\n",
        "  print(toBePredicted)\n",
        "  output = model(toBePredicted)\n",
        "  print(output)\n",
        "  #  Write your code here\n",
        "def cross_entropy(t, y):\n",
        "  e=0.00001 #To avoid log(0)=-inf\n",
        "  cross_entropy=  -t * np.log(y+e) - (1 - t) * np.log(1 - y+e)\n",
        "\n",
        "  return cross_entropy\n",
        "\n",
        "def cost(y, t):\n",
        "  return cross_entropy(t,y).mean()\n",
        " # return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def compute_accuracy(y, t):\n",
        "    y = np.round(y)\n",
        "    return np.sum(y == t) / len(y)\n",
        "\n",
        "def train_model(model, train_data,validation_data,test_data,\n",
        "                                hypers=[]):\n",
        "  #Hypers = [Cost, Optimizer, Learning Rate, Weight Decay,epochs,batch_size]\n",
        "  ##COSTS\n",
        "  if len(hypers) == 0:\n",
        "    return\n",
        "  epochs = hypers[4]\n",
        "  batch_size = hypers[5]\n",
        "\n",
        "  # Cost Functions\n",
        "  if hypers[0] == 'MSE':\n",
        "    criterion = nn.MSELoss()\n",
        "  elif hypers[0] == 'CE':\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "  elif hypers[0] == 'NLL':\n",
        "    criterion = nn.NLLLoss()\n",
        "  elif hypers[0] == 'BCEWithLogitsLoss':\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "  else:\n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "  ## OPTIMIZERS\n",
        "  if hypers[1] == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=hypers[2], momentum=0.9)\n",
        "  elif hypers[1] == 'RMS':\n",
        "      optimizer = optim.RMSprop(model.parameters(), lr=hypers[2], alpha=0.99, eps=1e-08, weight_decay=hypers[6], momentum=0, centered=False)\n",
        "  elif hypers[1] == 'ADAM':\n",
        "      optimizer = optim.Adam(model.parameters(), lr=hypers[2], weight_decay=hypers[3])\n",
        "  else:\n",
        "      optimizer = optim.Adamax(model.parameters(), lr=hypers[2], betas=(0.9, 0.999), eps=1e-08, weight_decay=hypers[3])\n",
        "\n",
        "  iters, losses = [], []\n",
        "  iters_sub, train_accs, val_accs = [], [], []\n",
        "  optimizer.zero_grad()\n",
        "  n = 0  # the number of iterations\n",
        "  for n in range(0,epochs):\n",
        "      for i in range(0, train_data[0].shape[0], batch_size):\n",
        "          #Clean the grad\n",
        "          optimizer.zero_grad()\n",
        "          # get the input and targets of a minibatch\n",
        "          xt, st = get_batch(train_data[0],train_data[1], i, i + batch_size)\n",
        "          xt = torch.Tensor(xt)\n",
        "          st = torch.Tensor(st).float()\n",
        "          print(xt.shape)\n",
        "          zs = model(xt)\n",
        "          loss = criterion(zs, st)  # compute the total loss\n",
        "        #  print(loss)\n",
        "          loss.backward()  # compute updates for each parameter\n",
        "          optimizer.step()  # make the updates for each parameter\n",
        "      losses.append(float(loss) / batch_size)  # compute average loss\n",
        "      iters.append(n)\n",
        "      iters_sub.append(n)\n",
        "      train_cost = float(loss.detach().numpy())\n",
        "      train_acc = estimate_accuracy_torch(model, train_data[0],train_data[1])\n",
        "      train_accs.append(train_acc)\n",
        "      val_acc = estimate_accuracy_torch(model, validation_data[0], validation_data[1])\n",
        "      val_accs.append(val_acc)\n",
        "  print(\"NN Accuracy on Validation Set: %.2f%%\" % (val_acc * 100))\n",
        "  print(\"NN Accuracy on Training Set: %.2f%%\" % (train_acc * 100))\n",
        "  print(\"NN Accuracy on Test Set: %.2f%%\" % (estimate_accuracy_torch(model, test_data[0], test_data[1]) * 100))\n",
        "  return iters, losses, iters_sub, train_accs, val_accs\n"
      ],
      "metadata": {
        "id": "15pBOmcH-4Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Models**\n",
        "\n",
        "We want to examine several architectures to determine which will be the most suitable one. \n",
        "\n",
        "First, We'll implement a standard 3-layers Neural Network, and see how the data is handled. we'll use SGD and BCE optimizer. \n",
        "\n",
        "** haara : Our labels is in 1 or 2, we reduce it by 1 so it will by binary output."
      ],
      "metadata": {
        "id": "5AsXT23Z-9Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First Network implementaiton : "
      ],
      "metadata": {
        "id": "2X4NBUhzDCTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstBasicNN(nn.Module):\n",
        "    def __init__(self, num_hidden=30,activation='ReLU',dropout=0,dropout_value=0.5,norm='None',number_of_layers=2):\n",
        "        super(FirstBasicNN, self).__init__()\n",
        "        self.input_size = 27  # input_size\n",
        "        self.hidden_size = num_hidden # hidden_size was 30\n",
        "        self.number_of_layers = number_of_layers  # in the implementation we start from 0\n",
        "        if activation == \"ReLU\":\n",
        "          self.activation = torch.nn.ReLU()\n",
        "        elif activation == \"Sigmoid\":\n",
        "            self.activation = torch.nn.Sigmoid()\n",
        "        elif activation == \"Tanh\":\n",
        "            self.activation = torch.nn.Tanh()\n",
        "        else:\n",
        "            self.activation = torch.nn.ReLU()\n",
        "        self.dropout_value = dropout_value\n",
        "        if dropout == 'Dropout':\n",
        "          self.droper = torch.nn.Dropout(self.dropout_value)\n",
        "        else: #Apply a 0 dropout on the data (an empty layer)\n",
        "            self.droper = nn.Dropout(0)\n",
        "        if norm==\"None\":\n",
        "            self.norm = nn.Dropout(0)\n",
        "        elif norm==\"BatchNorm1d\":\n",
        "            self.norm = nn.BatchNorm1d(self.hidden_size)\n",
        "        elif norm==\"LayerNorm\":\n",
        "            self.norm = nn.LayerNorm(self.hidden_size)\n",
        "        elif norm==\"GroupNorm\":\n",
        "            self.norm = nn.GroupNorm(self.hidden_size)\n",
        "        else:\n",
        "            self.norm = nn.Dropout(0)\n",
        "        for i in range(0,self.number_of_layers):\n",
        "            if i == 0:\n",
        "                setattr(self, 'linear'+str(i), nn.Linear(self.input_size, self.hidden_size))\n",
        "            elif i == self.number_of_layers-1:\n",
        "                setattr(self, 'linear'+str(i), nn.Linear(self.hidden_size, 2))\n",
        "            else:\n",
        "                setattr(self, 'linear'+str(i), nn.Linear(self.hidden_size, self.hidden_size))\n",
        "            \n",
        "    def weight_init(self, mean, std):\n",
        "      for m in self._modules:\n",
        "          if isinstance(m, nn.Linear):\n",
        "              m.weight.data.normal_(mean, std)\n",
        "              m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inp):\n",
        "        for i in range(0, self.number_of_layers):\n",
        "            if i == 0:\n",
        "                inp = getattr(self, 'linear' + str(i))(inp)\n",
        "                inp = self.activation(inp)\n",
        "                inp = self.droper(inp)\n",
        "                inp = self.norm(inp)\n",
        "            elif i == self.number_of_layers - 1:\n",
        "                inp = getattr(self, 'linear' + str(i))(inp)\n",
        "            else:\n",
        "                inp = getattr(self, 'linear' + str(i))(inp)\n",
        "                inp = self.activation(inp)\n",
        "                inp = self.droper(inp)\n",
        "                inp = self.norm(inp)\n",
        "        return inp\n",
        "        "
      ],
      "metadata": {
        "id": "ijEgnHS5DBYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FirstBasicNN(num_hidden=25)\n",
        " #Hypers = [Cost, Optimizer, Learning Rate, Weight Decay,epochs,batch_size]\n",
        "model.weight_init(mean=0, std=0.02)\n",
        "learning_curve_info = train_model(model, train,valid ,test,\n",
        "                            hypers=['CE','ADAM',0.001,0.000,100,5000])\n"
      ],
      "metadata": {
        "id": "fkA8Z3bb-9zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve(learning_curve_info)"
      ],
      "metadata": {
        "id": "hqgz7VMWHBwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current network, which consists of two layers with 30 hidden neurons, has achieved a test accuracy of 96% using Adam Optimizer and Cross Entropy Loss. This is considered to be a good result and there is no evidence of overfitting.\n",
        "\n",
        " The next step is to explore various cost functions, such as MSE, CrossEntropy, Negative Log Likelihood, and BCEWithLogitsLoss, in order to improve the performance of the network and compare the results.\n",
        "\n",
        "\n",
        " First, Lets define our grid search method"
      ],
      "metadata": {
        "id": "MarczvKoxLF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import itertools\n",
        "\n",
        "def print_params(params_dict):\n",
        "    ## Print the hyper paramters in readable way\n",
        "    print(\"Cost Function: \", params_dict['cost'])\n",
        "    print(\"Optimizer: \", params_dict['optimizer'])\n",
        "    print(\"Learning Rate: \", params_dict['learning_rate'])\n",
        "    print(\"Normalization: \", params_dict['normalization'])\n",
        "    print(\"Dropout: \", params_dict['dropout'])\n",
        "    print(\"Dropout Value: \", params_dict['dropout_value'])\n",
        "    print(\"Weight Decay: \", params_dict['weight_decay'])\n",
        "    print(\"Batch Size: \", params_dict['batch_size'])\n",
        "    print(\"Number of Epochs: \", params_dict['epochs'])\n",
        "    print(\"Size of Hidden Layers: \", params_dict['hidden_layers_size'])\n",
        "    print(\"Amount of Hidden Layers: \", params_dict['hidden_layers_size'])\n",
        "    print(\"Activation Function: \", params_dict['activation_function'])\n",
        "\n",
        "\n",
        "def grid_search(train_data, validation_data, test_data, hyperparameters):\n",
        "    #This Function Perform a grid search for the best hyper parameters for our model.\n",
        "    # Create a list of all possible combinations of hyperparameters given a dictionary of hyperparameters\n",
        "    combinations = list(itertools.product(*hyperparameters.values()))\n",
        "    best_val_acc = 0\n",
        "    best_hyperparameters = None\n",
        "    # Iterate through all combinations of hyperparameters\n",
        "    for combination in combinations:\n",
        "        print(\"*\"*10 + \"START\" + \"*\"*10)\n",
        "        # Convert the combination tuple to a dictionary\n",
        "        hypers = dict(zip(hyperparameters.keys(), combination))\n",
        "        print_params(hypers)\n",
        "        model = FirstBasicNN(num_hidden=hypers['hidden_layers_size'], activation=hypers['activation_function'], dropout=hypers['dropout'], \n",
        "                             dropout_value=hypers['dropout_value'], norm=hypers['normalization'],\n",
        "                             number_of_layers=hypers['hidden_layers'])\n",
        "        #Initlize all the weights\n",
        "        model.weight_init(mean=0, std=0.02)\n",
        "        # Train the model with the current combination of hyperparameters\n",
        "        aranged_hypers = [hypers['cost'], hypers['optimizer'], hypers['learning_rate'], hypers['weight_decay'], hypers['epochs'], hypers['batch_size']]\n",
        "        #Get only the validation accuracy as our score\n",
        "        _, _, _, _, val_accs = train_model(model, train_data, validation_data, test_data, hypers=aranged_hypers)\n",
        "        # Get the last validation accuracy from the training process\n",
        "        val_acc = val_accs[-1]\n",
        "        # Update the best hyperparameters if the current combination results in a higher accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_hyperparameters = combination\n",
        "        print(\"*\"*10 + \"END\" + \"*\"*10)\n",
        "\n",
        "    # Print the best hyperparameters and their corresponding validation accuracy\n",
        "    print(\"Best hyperparameters:\", best_hyperparameters)\n",
        "    print(\"Best validation accuracy: %.2f%%\" % (best_val_acc * 100))\n",
        "    return best_hyperparameters\n",
        "\n",
        "\n",
        "param_grid = {'cost': ['MSE', 'CE', 'NLL', 'BCEWithLogitsLoss'],\n",
        "              'optimizer': ['SGD', 'RMS', 'ADAM', 'ADAMAX'],\n",
        "              'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
        "              'normalization': ['BatchNorm1d',\"GroupNorm\", 'LayerNorm', 'None'],\n",
        "              'dropout': ['Dropout', 'None'],\n",
        "              'dropout_value': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "              'weight_decay': [0, 0.0000001, 0.000001, 0.00001, 0.01, 1],\n",
        "              'batch_size': [10, 20, 30, 40, 50],\n",
        "              'epochs': [50, 100, 130, 160, 200, 300],\n",
        "              'hidden_layers_size': [10, 20, 30, 40, 50],\n",
        "              'hidden_layers': [2, 3, 4, 5, 6, 7],\n",
        "              'activation_function': ['ReLU', 'Sigmoid', 'Tanh'],\n",
        "              'batch_size': [500, 1000, 5000]\n",
        "              }\n",
        "grid_search(train, valid, test, param_grid)\n"
      ],
      "metadata": {
        "id": "aAOcpIQvx-JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wIm_PQTZx88P"
      }
    }
  ]
}